{\rtf1\ansi\ansicpg1252\cocoartf2706
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fmodern\fcharset0 Courier-Oblique;\f1\fmodern\fcharset0 Courier;\f2\fmodern\fcharset0 Courier-Bold;
\f3\fnil\fcharset0 HelveticaNeue;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\i\fs28 \cf0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 # load the data and packages needed
\f1\i0 \
\pard\pardeftab720\partightenfactor0

\f2\b \cf0 import
\f1\b0  pandas 
\f2\b as
\f1\b0  pd\

\f2\b import
\f1\b0  numpy 
\f2\b as
\f1\b0  np\

\f2\b import
\f1\b0  pandas 
\f2\b as
\f1\b0  pd\

\f2\b import
\f1\b0  matplotlib.pyplot 
\f2\b as
\f1\b0  plt\

\f2\b from
\f1\b0  sklearn.ensemble 
\f2\b import
\f1\b0  RandomForestClassifier\

\f2\b from
\f1\b0  sklearn 
\f2\b import
\f1\b0  tree\

\f2\b from
\f1\b0  sklearn.tree 
\f2\b import
\f1\b0  DecisionTreeClassifier\

\f2\b from
\f1\b0  sklearn.naive_bayes 
\f2\b import
\f1\b0  GaussianNB\

\f2\b from
\f1\b0  sklearn.ensemble 
\f2\b import
\f1\b0  AdaBoostClassifier, VotingClassifier\

\f2\b from
\f1\b0  sklearn.linear_model 
\f2\b import
\f1\b0  LogisticRegression\

\f2\b from
\f1\b0  sklearn.neighbors 
\f2\b import
\f1\b0  KNeighborsClassifier\

\f2\b from
\f1\b0  sklearn.metrics 
\f2\b import
\f1\b0  confusion_matrix\

\f2\b from
\f1\b0  sklearn.metrics 
\f2\b import
\f1\b0  classification_report\

\f2\b from
\f1\b0  sklearn.metrics 
\f2\b import
\f1\b0  accuracy_score\

\f2\b from
\f1\b0  sklearn.model_selection 
\f2\b import
\f1\b0  train_test_split\
\
\pard\pardeftab720\partightenfactor0

\f0\i \cf0 # load the dataset
\f1\i0 \
spam_data 
\f2\b =
\f1\b0  pd
\f2\b .
\f1\b0 read_csv('spam.csv')\
\pard\pardeftab720\qr\partightenfactor0

\f3 \cf0 In\'a0[\'a0]:\
\pard\pardeftab720\partightenfactor0

\f0\i \cf0 # Splitting Dataset into two parts train and testing
\f1\i0 \

\f0\i # Training Set = 1000
\f1\i0 \

\f0\i # Test Set = 3601
\f1\i0 \
\
\pard\pardeftab720\partightenfactor0

\f2\b \cf0 from
\f1\b0  sklearn.model_selection 
\f2\b import
\f1\b0  train_test_split\
spam_test_set, spam_training_set 
\f2\b =
\f1\b0  train_test_split(spam_data, test_size
\f2\b =
\f1\b0 0.8,random_state
\f2\b =
\f1\b0 42)\
\pard\pardeftab720\qr\partightenfactor0

\f3 \cf0 In\'a0[\'a0]:\
\pard\pardeftab720\partightenfactor0

\f0\i \cf0 # Creating Training and Test Targets
\f1\i0 \
spam_training_data, spam_training_target 
\f2\b =
\f1\b0  spam_training_set[["make","address","all","3d","our","over","remove","internet","order","mail","receive","will","people","report","addresses","free","business","email","you","credit","your","font","0","money","hp","hpl","george","650","lab","labs","telnet","857","data","415","85","technology","1999","parts","pm","direct","cs","meeting","original","project","re","edu","table","conference","semicol","paren","bracket","bang","dollar","pound","cap_avg","cap_long","cap_total"]], spam_training_set["Class"]\
spam_test_data, spam_test_target 
\f2\b =
\f1\b0  spam_test_set[["make","address","all","3d","our","over","remove","internet","order","mail","receive","will","people","report","addresses","free","business","email","you","credit","your","font","0","money","hp","hpl","george","650","lab","labs","telnet","857","data","415","85","technology","1999","parts","pm","direct","cs","meeting","original","project","re","edu","table","conference","semicol","paren","bracket","bang","dollar","pound","cap_avg","cap_long","cap_total"]], spam_test_set["Class"]\
\pard\pardeftab720\qr\partightenfactor0

\f3 \cf0 In\'a0[\'a0]:\
\pard\pardeftab720\partightenfactor0

\f0\i \cf0 # Create Classifier Objects
\f1\i0 \
\
clf_dt 
\f2\b =
\f1\b0  DecisionTreeClassifier(criterion
\f2\b =
\f1\b0 "entropy")\

\f0\i #clf_ad = AdaBoostClassifier(n_estimators = 1000, base_estimator = clf_dt)
\f1\i0 \

\f0\i #clf_rf = RandomForestClassifier(n_estimators = 1000, max_features = 'sqrt')
\f1\i0 \
clf_lr 
\f2\b =
\f1\b0  LogisticRegression(max_iter
\f2\b =
\f1\b0 4000)\
clf_gnb 
\f2\b =
\f1\b0  GaussianNB()\
fused_model 
\f2\b =
\f1\b0  VotingClassifier(estimators 
\f2\b =
\f1\b0  [('DT', clf_dt),('LR',clf_lr),('GNB',clf_gnb)], voting 
\f2\b =
\f1\b0 'hard')\
\
fused_model
\f2\b .
\f1\b0 fit(spam_training_data,spam_training_target)\
spam_test_target_predict 
\f2\b =
\f1\b0  fused_model
\f2\b .
\f1\b0 predict(spam_test_data)\
\pard\pardeftab720\qr\partightenfactor0

\f3 \cf0 In\'a0[\'a0]:\
\pard\pardeftab720\partightenfactor0

\f0\i \cf0 # This is code to print the confusion matrix in a better way 
\f1\i0 \

\f0\i # I used the sklearn site https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html
\f1\i0 \
\
\pard\pardeftab720\partightenfactor0

\f2\b \cf0 from
\f1\b0  sklearn.metrics 
\f2\b import
\f1\b0  confusion_matrix\

\f2\b import
\f1\b0  seaborn 
\f2\b as
\f1\b0  sns\
cf_matrix 
\f2\b =
\f1\b0  confusion_matrix(spam_test_target,spam_test_target_predict)\
image 
\f2\b =
\f1\b0  sns
\f2\b .
\f1\b0 heatmap(cf_matrix, annot
\f2\b =True
\f1\b0 , fmt
\f2\b =
\f1\b0 'g', cmap
\f2\b =
\f1\b0 'Blues')\
image
\f2\b .
\f1\b0 set_title('Fused Classifier 80-20 Split\\n\\n');\
image
\f2\b .
\f1\b0 set_xlabel('\\nPredicted Values')\
image
\f2\b .
\f1\b0 set_ylabel('Actual Values ');\
image
\f2\b .
\f1\b0 xaxis
\f2\b .
\f1\b0 set_ticklabels(['False','True'])\
image
\f2\b .
\f1\b0 yaxis
\f2\b .
\f1\b0 set_ticklabels(['False','True'])\
\
plt
\f2\b .
\f1\b0 show()\
\pard\pardeftab720\qr\partightenfactor0

\f3 \cf0 In\'a0[\'a0]:\
\pard\pardeftab720\partightenfactor0

\f1 \cf0 print(confusion_matrix(spam_test_target,spam_test_target_predict))\
print(classification_report(spam_test_target,spam_test_target_predict))\
print(accuracy_score(spam_test_target,spam_test_target_predict))\
}